---
title: "Group final project - BAN400 fall 2020 "
author: "Candidate: (...)"
date: "6/11/2020"
output:
  pdf_document:
    dev: png
  keep_tex: yes
  word_document: default
header-includes:
- \usepackage{dcolumn}
- \usepackage{booktabs}
- \usepackage{sectsty} \sectionfont{\centering}
- \usepackage{indentfirst}
fontsize: 11pt
spacing: double
indent: true
---


```{r setup, include=FALSE, message=FALSE}

knitr::opts_chunk$set(echo = FALSE)
options(scipen = 99999) # Do not wish to have scientific  notations.

# loading the required packages:

library(tidyverse)
library(parsnip)
library(yardstick)
library(rsample)
require(doParallel)
library(recipes)
library(ggridges)
library(ggplot2)
library(patchwork)
library(xgboost)
library(tidymodels)
library(FactoMineR)
library(tibble)
library(tm)
library(tidytext)
require(sentimentr)
library(SentimentAnalysis)
library(lexicon)
require(tokenizers)

```


## Workflow: 

### 1. get the data to run in R and tidyverse
  
#### 1.0 merge true and fake news datasets, adding a new dummy column if news is true or fake. 
  
```{r, echo=FALSE}

# Loading in the data, binding them together and removing the unbinded to keep environment clean----

df_fake <- readRDS("Fake.rds") %>%
  mutate(Fake = 1)


df_true <- readRDS("True.rds") %>% 
  mutate(Fake = 0)


DF <- rbind(df_fake, df_true)

rm(df_true, df_fake)



```

#### 1.1 Prepossesing and getting the data ready for modelling. 

---- **Cleaning the data** ----

> Here we do some simple prepossesing steps, like removing characters and words that don't carry any meaning to our analysis. 

```{r}

# First we'd like to extract a few variables which we might use on our predictive models

DF <- DF %>% 
  mutate(w_count.txt = str_count(paste(DF$text), " "),
         punct_count.txt = str_count(paste(DF$text), "[[:punct:]]"),
         exclamation_count.txt = str_count(paste(DF$text), "!"), 
         w_count.title = str_count(paste(DF$title), " "),
         punct_count.title = str_count(paste(DF$title), "[[:punct:]]"),
         exclamation_count.title = str_count(paste(DF$title), "!"))



# Now we do some prepossessing of the data

DF <-  DF %>% 
  mutate(text = text %>% tolower() %>%
             gsub("[^[:alpha:]]", " ", .) %>%
#this removes any character that's not alphabetic. Those usually not carry any meaning. We may want to analyse them individually.
             gsub('\\b\\w{1,2}\\s|\\b\\w{21,}\\s','', .) %>%
#removing words that are too small (from 1 or 2 letters and huge words, 21+ letters)
             removeWords(., c(stopwords(kind = "en"))),
# removing stopwords, which are words that are most common in the English language, yet carry no meaning. These are found inside the stopwords dictionary int he tm package
         title = title %>% tolower() %>% 
             gsub("[^[:alpha:]]", " ", .) %>% 
             gsub('\\b\\w{1,2}\\s|\\b\\w{21,}\\s','', .) %>% 
             removeWords(., c(stopwords(kind = "en"))))


```

---- **Tokenizing the data and creating bigrams** ----

> Here we will split the data word by word. We will do the analysis with unigrams (which is every word separated individually), and bigrams (which are words separated in twos). They may give us slightly different topics, though should be somewhat similar, so its important to check for overfitting if using both in a regression. 

```{r}
# Tokenizing

# Tokenizing with bigrams. What this does to us, is that as bigrams, words carry a different meaning as the paired words may carry a different meaning. 

DF <-  DF %>% 
  mutate(bigram_text = tokenize_ngrams(text , n = 2, ngram_delim = "_"),
         bigram_title = tokenize_ngrams(title , n = 2, ngram_delim = "_"),
         token_text = tokenize_ngrams(text , n = 1, ngram_delim = ""),
         token_title = tokenize_ngrams(title , n = 1, ngram_delim = ""))



```



### 2. Separate the data into training and test data.


```{r}

# Setting a seed so the experiment is replicable

set.seed(10)

# Creating test and train data:

sample <- initial_split(DF,
                           strata = Fake, # variable used for stratified sampling
                           prop = 0.75)# We are splitting 75% training and 25% test.

# Extracting the training data: 

train<-training(sample)

# Extracting the test data:

test<-testing(sample)

```


### 3 Visualizing the data


*Visualizing the sentiment scores*

```{r}

# Creating a dataframe with all the words

all_text <- paste(c(train$text), collapse = " ")

#tokenizing all words

text_tokens <- scan(text = all_text,
                      what = "character",
                      quote = "")

#Making a frequency table of all words

freq_text <- table(text_tokens)%>% 
  sort(decreasing = T) %>% 
  as.data.frame()

# choosing the sentiment dictionary 

dictionary <- lexicon::hash_sentiment_sentiword
dictionary.n <- dictionary%>% filter(dictionary$y < 0)
dictionary.p <- dictionary%>% filter(dictionary$y > 0)

#seeing which positive words are present

positive_words <- freq_text %>%
  filter(text_tokens %in% dictionary.p$x) %>%
  arrange(desc(Freq)) %>% head(50)

# seeing which negative words are present

negative_words <- freq_text %>%
  filter(text_tokens %in% dictionary.n$x) %>%
  arrange(desc(Freq)) %>% head(50)


```



*splitting the clean data back into true and false for the visualization*

```{r}

f.train <- train  %>% filter(train$Fake == 1)

t.train <- train  %>% filter(train$Fake == 0)


# extracting all the words from our training data to create the topic modeling

f.all_text <- paste(c(f.train$text), collapse = " ")

f.all_title <- paste(c(f.train$title), collapse = " ")

t.all_text <- paste(c(t.train$text), collapse = " ")

t.all_title <- paste(c(t.train$title), collapse = " ")

# splitting the words and tokenizing them.

f.text_tokens <- scan(text = f.all_text,
                      what = "character",
                      quote = "")

f.title_tokens <- scan(text = f.all_title,
                      what = "character",
                      quote = "")

f.text_bigrams <- paste(unlist(f.train$bigram_text), collapse = " ") %>% 
  strsplit(.," ") %>% unlist

f.title_bigrams <- paste(unlist(f.train$bigram_title), collapse = " ") %>% 
  strsplit(.," ") %>%  unlist

t.text_tokens <- scan(text = t.all_text,
                      what = "character",
                      quote = "")

t.title_tokens <- scan(text = t.all_title,
                      what = "character",
                      quote = "")

t.text_bigrams <- paste(unlist(t.train$bigram_text), collapse = " ") %>% 
  strsplit(.," ") %>% unlist

t.title_bigrams <- paste(unlist(t.train$bigram_title), collapse = " ") %>% 
  strsplit(.," ") %>%  unlist
  

```
#### 3.1 Making a KWIC (Key words in context) table

> *this helps us better visualize the text and understand the context of some words*


```{r}


# Getting a KWIC matrix
# Here we build the function "get.KWIC" in order to inspect keywords. The KWIC matrix helps us identify keywords and the words related to it in our corpus. It aids in visualizing the data. 

### building the function ###

get.KWIC <- function(keyword,
                     n,
                     tokenized_text,
                     cores){
  cores <- min(detectCores(), cores)
  cl <- makeCluster(cores)
  registerDoParallel(cl)
  
  # this helps us seek the keyword
  index.env <- grep(keyword, tokenized_text)
  
  
  # this creates the table for us as a tibble. 
  KWIC <- tibble(left = parSapply(cl, index.env,
                               function(i) {ifelse( i-n >0,
                                 paste(tokenized_text[i-n:1],
                                                  collapse = " "),NA)}),
                 keyword = tokenized_text[index.env],
                 right = parSapply(cl, index.env,
                                function(i) {paste(tokenized_text[i+1:n],
                                                   collapse = " ")}))
  
  stopCluster(cl)
  # return
  return(KWIC)

}


```



```{r}


### using the function ###

get.KWIC(keyword = "trump",
         n = 2,
         tokenized_text = f.text_tokens, 8) -> f.trump

get.KWIC(keyword = "donald_trump",
         n = 2,
         tokenized_text = f.text_bigrams, 8) -> f.trump_bigram

get.KWIC(keyword = "trump",
         n = 2,
         tokenized_text = t.text_tokens, 8) -> t.trump

get.KWIC(keyword = "donald_trump",
         n = 2,
         tokenized_text = t.text_bigrams, 8) -> t.trump_bigram

## we just picked these topics since they are polarizing, but they are only here as an example of the function at work ##



```

#### 3.2 Visualizing word frequency

```{r}

#making frequency tables out of the fake news

f.freq_text_1 <- table(f.text_tokens)%>% 
  sort(decreasing = T) %>% 
  as.data.frame()

f.freq_text_2 <- table(f.text_bigrams)%>% 
  sort(decreasing = T) %>% 
  as.data.frame()

f.freq_title_1 <- table(f.title_tokens)%>% 
  sort(decreasing = T) %>% 
  as.data.frame()

f.freq_title_2 <- table(f.title_bigrams)%>% 
  sort(decreasing = T) %>% 
  as.data.frame()

#making frequency tables out of the true news

t.freq_text_1 <- table(t.text_tokens)%>% 
  sort(decreasing = T) %>% 
  as.data.frame()

t.freq_text_2 <- table(t.text_bigrams)%>% 
  sort(decreasing = T) %>% 
  as.data.frame()

t.freq_title_1 <- table(t.title_tokens)%>% 
  sort(decreasing = T) %>% 
  as.data.frame()

t.freq_title_2 <- table(t.title_bigrams)%>% 
  sort(decreasing = T) %>% 
  as.data.frame()


```

**Fake News word frequency**

```{r,fig.height=4,fig.width=4,warning=FALSE,message=FALSE}

#Making wordclouds of our training data

require(wordcloud)


wordcloud(words = f.freq_text_1$f.text_tokens [1:50],
          freq = f.freq_text_1$Freq[1:50])

```
**True News word frequency**

```{r,fig.height=4,fig.width=4,warning=FALSE,message=FALSE}

wordcloud(words = t.freq_text_1$t.text_tokens [1:50],
          freq = t.freq_text_1$Freq[1:50])


```

**Fake News bigram frequency**

```{r,fig.height=4,fig.width=4,warning=FALSE,message=FALSE}

wordcloud(words = f.freq_text_2$f.text_bigrams [1:50],
          freq = f.freq_text_2$Freq[1:50])

```

**True News bigram frequency**

```{r,fig.height=4,fig.width=4,warning=FALSE,message=FALSE}

wordcloud(words = t.freq_text_2$t.text_bigrams [1:50],
          freq = t.freq_text_2$Freq[1:50])

```

#### Checking for words that are unique to the fakenews


```{r}

```


### 4. Using the sensitivity analysis to make usable columns in a tibble for each news article. 

```{r}

```



### 5. Training the machine learning model



### 6. Test the model, and polish it (maybe go back to step 2 if need be)

```{r}

```


### 7. make a shiny app, which will work as an interface to plug in new data and see if the news stories is true or fake. 

```{r}

```


####  7.1 make the app more user friendly

```{r}

```


