---
title: "Group final project - BAN400 fall 2020 "
author: "Candidate: (...)"
date: "6/11/2020"
output:
  pdf_document:
    dev: png
  keep_tex: yes
  word_document: default
header-includes:
- \usepackage{dcolumn}
- \usepackage{booktabs}
- \usepackage{sectsty} \sectionfont{\centering}
- \usepackage{indentfirst}
fontsize: 11pt
spacing: double
indent: true
---


```{r setup, include=FALSE, message=FALSE}

knitr::opts_chunk$set(echo = FALSE)
options(scipen = 99999) # Do not wish to have scientific  notations.

# loading the required packages:

library(tidyverse)
library(parsnip)
library(yardstick)
library(rsample)
require(doParallel)
library(recipes)
library(ggridges)
library(ggplot2)
library(patchwork)
library(xgboost)
library(tidymodels)
library(FactoMineR)
library(tibble)
library(tm)
library(tidytext)
require(sentimentr)
library(SentimentAnalysis)
library(lexicon)
require(tokenizers)

```


\tableofcontents

\pagebreak

# Project suggestion Group 30

## Initial proposal: 

*"Textual data analysis combined with regression analysis using data on fake/true news"*

## Method: 

- We create regressors based on the words present in the fake and real news. So far we can create a sentiment factor, on both the body and the title of the article, and a keyword per word factor, also both on the title and the body. 
 
- Then use a predictive model to see if the news are real or fake based on these factors. We can run a factor regression or perhaps a machine learning model, like an XGBoost model.

- Finally, we can create a "shiny ap" which allows up to paste news articles in, the ap will then preprocess the article and give us a score of "fake probability". 

## Analysis:

 - Tokenizing and prepossesing before doing any textual analysis, we need to preposess the data, which mean shaping it in order for the different models to read them. 

 - Topic modelling: Using the words in the dataset to define a concrete topic in each of the files. This could help us see which words to look out for, perhaps there is a topic which is "Hype up words", that might be more present in the fake news. We can create a "topic per word" score. 
 
 - Sentiment analysis to gather data on how negative /positive the fake news is compared with true news. Maybe there is a specific sentiment in the fake news that we can extract, we can add these numbers to our regression, to make a predictive model to see if the news are fake or not. 
 
 - Run a regression, were 1 is fake news and 0 is true news. It could be a linear regression, or we could experiment with some machine learning model. We keep whichever performs best. 

## Sources

For this we will use a data set from Kaggle: https://www.kaggle.com/clmentbisaillon/fake-and-real-news-dataset 
(Links to an external site.) with data from 2016 to 2017.

All graphs will be visualized using ggplot package.

 - Use machine learning
 - Apply it to live data
 
https://www.r-bloggers.com/2020/10/sentiment-analysis-in-r-with-custom-lexicon-dictionary-using-tidytext/"


https://rstudio-education.github.io/tidyverse-cookbook/import.html
https://www.tidyverse.org/blog/2020/06/recipes-0-1-13/


*check out vip package 

## Workflow: 

### 1. get the data to run in R and tidyverse
  
#### 1.0 merge true and fake news datasets, adding a new dummy column if news is true or fake. 
  
```{r, echo=FALSE}

# Loading in the data, binding them together and removing the unbinded to keep environment clean----

df_fake <- readRDS("Fake.rds") %>%
  mutate(Fake = 1)


df_true <- readRDS("True.rds") %>% 
  mutate(Fake = 0)


DF <- rbind(df_fake, df_true)

rm(df_true, df_fake)

```

#### 1.1 separate the data into training and test data.

```{r}

# Setting a seed so the experiment is replicable

set.seed(10)

# Creating test and train data:

sample <- initial_split(DF,
                           strata = Fake, # variable used for stratified sampling
                           prop = 0.75)# We are splitting 75% training and 25% test.

# Extracting the training data: 

train<-training(sample)

# Extracting the test data:

test<-testing(sample)

```


### 2. Prepossesing and getting the data ready for modelling. 


---- **Cleaning the data** ----

> Here we do some simple prepossesing steps, like removing characters and words that don't carry any meaning to our analysis. 

```{r}

train <-  train %>% 
  mutate(text = text %>% tolower() %>%
             gsub("[^[:alpha:]]", " ", .) %>%
#this removes any character that's not alphabetic. Those usually not carry any meaning. We may want to analyse them individually.
             gsub('\\b\\w{1,2}\\s|\\b\\w{21,}\\s','', .) %>%
#removing words that are too small (from 1 or 2 letters and huge words, 21+ letters)
             removeWords(., c(stopwords(kind = "en"))),
# removing stopwords, which are words that are most common in the English language, yet carry no meaning. These are found inside the stopwords dictionary int he tm package
         title = title %>% tolower() %>% 
             gsub("[^[:alpha:]]", " ", .) %>% 
             gsub('\\b\\w{1,2}\\s|\\b\\w{21,}\\s','', .) %>% 
             removeWords(., c(stopwords(kind = "en"))))


```

---- **Tokenizing the data and creating bigrams** ----

> Here we will split the data word by word. We will do the analysis with unigrams (which is every word separated individually), and bigrams (which are words separated in twos). They may give us slightly different topics, though should be somewhat similar, so its important to check for overfitting if using both in a regression. 

```{r}
# Tokenizing

# Tokenizing with bigrams. What this does to us, is that as bigrams, words carry a different meaning as the paired words may carry a different meaning. 

train <-  train %>% 
  mutate(bigram_text = tokenize_ngrams(text , n = 2, ngram_delim = "_"),
         bigram_title = tokenize_ngrams(title , n = 2, ngram_delim = "_"),
         token_text = tokenize_ngrams(text , n = 1, ngram_delim = ""),
         token_title = tokenize_ngrams(title , n = 1, ngram_delim = ""))



```



#### 2.1 cleaning the words, and making them ready for the sensitivy analysis(making a frequency list)

```{r}

```


#### 2.1.1 Use recipes in order to make this cleaning process             replicable for test data 

```{r}

```


### 3 Visualizing the data

*splitting the clean data back into true and false for the visualization*

```{r}

f.train <- train  %>% filter(train$Fake == 1)

t.train <- train  %>% filter(train$Fake == 0)


# extracting all the words from our training data to create the topic modeling

f.all_text <- paste(c(f.train$text), collapse = " ")

f.all_title <- paste(c(f.train$title), collapse = " ")

t.all_text <- paste(c(t.train$text), collapse = " ")

t.all_title <- paste(c(t.train$title), collapse = " ")

# splitting the words and tokenizing them.

f.text_tokens <- scan(text = f.all_text,
                      what = "character",
                      quote = "")

f.title_tokens <- scan(text = f.all_title,
                      what = "character",
                      quote = "")

f.text_bigrams <- paste(unlist(f.train$bigram_text), collapse = " ") %>% 
  strsplit(.," ") %>% unlist

f.title_bigrams <- paste(unlist(f.train$bigram_title), collapse = " ") %>% 
  strsplit(.," ") %>%  unlist

t.text_tokens <- scan(text = t.all_text,
                      what = "character",
                      quote = "")

t.title_tokens <- scan(text = t.all_title,
                      what = "character",
                      quote = "")

t.text_bigrams <- paste(unlist(t.train$bigram_text), collapse = " ") %>% 
  strsplit(.," ") %>% unlist

t.title_bigrams <- paste(unlist(t.train$bigram_title), collapse = " ") %>% 
  strsplit(.," ") %>%  unlist
  

```
#### 3.1 Making a KWIC (Key words in context) table

> *this helps us better visualize the text and understand the context of some words*


```{r}


# Getting a KWIC matrix
# Here we build the function "get.KWIC" in order to inspect keywords. The KWIC matrix helps us identify keywords and the words related to it in our corpus. It aids in visualizing the data. 

### building the function ###

get.KWIC <- function(keyword,
                     n,
                     tokenized_text,
                     cores){
  cores <- min(detectCores(), cores)
  cl <- makeCluster(cores)
  registerDoParallel(cl)
  
  # this helps us seek the keyword
  index.env <- grep(keyword, tokenized_text)
  
  
  # this creates the table for us as a tibble. 
  KWIC <- tibble(left = parSapply(cl, index.env,
                               function(i) {ifelse( i-n >0,
                                 paste(tokenized_text[i-n:1],
                                                  collapse = " "),NA)}),
                 keyword = tokenized_text[index.env],
                 right = parSapply(cl, index.env,
                                function(i) {paste(tokenized_text[i+1:n],
                                                   collapse = " ")}))
  
  stopCluster(cl)
  # return
  return(KWIC)

}


```



```{r}


### using the function ###

get.KWIC(keyword = "trump",
         n = 2,
         tokenized_text = f.text_tokens, 8) -> f.trump

get.KWIC(keyword = "donald_trump",
         n = 2,
         tokenized_text = f.text_bigrams, 8) -> f.trump_bigram

get.KWIC(keyword = "trump",
         n = 2,
         tokenized_text = t.text_tokens, 8) -> t.trump

get.KWIC(keyword = "donald_trump",
         n = 2,
         tokenized_text = t.text_bigrams, 8) -> t.trump_bigram

## we just picked these topics since they are polarizing, but they are only here as an example of the function at work ##



```

#### 3.2 Visualizing word frequency

```{r}

#making frequency tables out of the fake news

f.freq_text_1 <- as.data.frame(table(f.text_tokens))%>% 
  sort(decreasing = T)

f.freq_text_2 <- as.data.frame(table(f.text_bigrams))%>% 
  sort(decreasing = T)

f.freq_title_1 <- as.data.frame(table(f.title_tokens))%>% 
  sort(decreasing = T)

f.freq_title_2 <- as.data.frame(table(f.title_bigrams))%>% 
  sort(decreasing = T)

#making frequency tables out of the true news

t.freq_text_1 <- as.data.frame(table(t.text_tokens))%>% 
  sort(decreasing = T)

t.freq_text_2 <- as.data.frame(table(t.text_bigrams))%>% 
  sort(decreasing = T)

t.freq_title_1 <- as.data.frame(table(t.title_tokens))%>% 
  sort(decreasing = T)

t.freq_title_2 <- as.data.frame(table(t.title_bigrams))%>% 
  sort(decreasing = T)


```



### 4. Using the sensitivity analysis to make usable columns in a tibble for each news article. 

```{r}

```



### 5. Training the machine learning model

```{r}

```


### 6. Test the model, and polish it (maybe go back to step 2 if need be)

```{r}

```


### 7. make a shiny app, which will work as an interface to plug in new data and see if the news stories is true or fake. 

```{r}

```


####  7.1 make the app more user friendly

```{r}

```



