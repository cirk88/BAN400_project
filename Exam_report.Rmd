---
title: "Group final project - BAN400 fall 2020 "
author: "Candidate: (...)"
date: "6/11/2020"
output:
  pdf_document:
    dev: png
  keep_tex: yes
  word_document: default
header-includes:
- \usepackage{dcolumn}
- \usepackage{booktabs}
- \usepackage{sectsty} \sectionfont{\centering}
- \usepackage{indentfirst}
fontsize: 11pt
spacing: double
indent: true
---


```{r setup, include=FALSE, message=FALSE}

knitr::opts_chunk$set(echo = FALSE)
options(scipen = 99999) # Do not wish to have scientific  notations.

# loading the required packages:

library(tidyverse)
library(parsnip)
library(yardstick)
library(rsample)
require(doParallel)
library(recipes)
library(ggridges)
library(ggplot2)
library(patchwork)
library(xgboost)
library(tidymodels)
library(FactoMineR)
library(tibble)
library(tm)
library(tidytext)
require(sentimentr)
library(SentimentAnalysis)
library(lexicon)
require(tokenizers)

```


```{r}

### IMPORTANT! ###

#set the number of cores you will use for parallel calculation. Do not exceed the recommended amount.

cl <- parallel::makeCluster(8)

```


\tableofcontents

\pagebreak

# Project suggestion Group 30

## Initial proposal: 

*"Textual data analysis combined with regression analysis using data on fake/true news"*

## Method: 

- We create regressors based on the words present in the fake and real news. So far we can create a sentiment factor, on both the body and the title of the article, and a keyword per word factor, also both on the title and the body. 
 
- Then use a predictive model to see if the news are real or fake based on these factors. We can run a factor regression or perhaps a machine learning model, like an XGBoost model.

- Finally, we can create a "shiny ap" which allows up to paste news articles in, the ap will then preprocess the article and give us a score of "fake probability". 

## Analysis:

 - Tokenizing and prepossesing before doing any textual analysis, we need to preposess the data, which mean shaping it in order for the different models to read them. 

 - Topic modelling: Using the words in the dataset to define a concrete topic in each of the files. This could help us see which words to look out for, perhaps there is a topic which is "Hype up words", that might be more present in the fake news. We can create a "topic per word" score. 
 
 - Sentiment analysis to gather data on how negative /positive the fake news is compared with true news. Maybe there is a specific sentiment in the fake news that we can extract, we can add these numbers to our regression, to make a predictive model to see if the news are fake or not. 
 
 - Run a regression, were 1 is fake news and 0 is true news. It could be a linear regression, or we could experiment with some machine learning model. We keep whichever performs best. 

## Sources

For this we will use a data set from Kaggle: https://www.kaggle.com/clmentbisaillon/fake-and-real-news-dataset 
(Links to an external site.) with data from 2016 to 2017.

All graphs will be visualized using ggplot package.

 - Use machine learning
 - Apply it to live data
 
https://www.r-bloggers.com/2020/10/sentiment-analysis-in-r-with-custom-lexicon-dictionary-using-tidytext/"


https://rstudio-education.github.io/tidyverse-cookbook/import.html
https://www.tidyverse.org/blog/2020/06/recipes-0-1-13/


*check out vip package 

## Workflow: 

### 1. get the data to run in R and tidyverse
  
#### 1.0 merge true and fake news datasets, adding a new dummy column if news is true or fake. 
  
```{r, echo=FALSE}

# Loading in the data, binding them together and removing the unbinded to keep environment clean----

df_fake <- readRDS("Fake.rds") %>%
  mutate(Fake = 1)


df_true <- readRDS("True.rds") %>% 
  mutate(Fake = 0)


DF <- rbind(df_fake, df_true)

rm(df_true, df_fake)


```

#### 1.1 Prepossesing and getting the data ready for modelling. 

---- **Cleaning the data** ----

> Here we do some simple prepossesing steps, like removing characters and words that don't carry any meaning to our analysis. 

```{r}

# First we'd like to extract a few variables which we might use on our predictive models

DF <- DF %>% 
  mutate(w_count.txt = str_count(paste(DF$text), " "),
         punct_count.txt = str_count(paste(DF$text), "[[:punct:]]"),
         upper_count.txt = str_count(paste(DF$text), "[[:upper:]]"),
         exclamation_count.txt = str_count(paste(DF$text), "!"), 
         w_count.title = str_count(paste(DF$title), " "),
         punct_count.title = str_count(paste(DF$title), "[[:punct:]]"),
         upper_count.title = str_count(paste(DF$title), "[[:upper:]]"),
         exclamation_count.title = str_count(paste(DF$title), "!"))


# Now we do some prepossessing of the data

DF <-  DF %>%
  filter( w_count.title > 0 ) %>%
  filter( w_count.txt > 30 ) %>% 
  mutate(text = text %>% tolower() %>%
             gsub("[^[:alpha:]]", " ", .) %>%
#this removes any character that's not alphabetic. Those usually not carry any meaning. We may want to analyse them individually.
             gsub('\\b\\w{1,2}\\s|\\b\\w{21,}\\s','', .) %>%
#removing words that are too small (from 1 or 2 letters and huge words, 21+ letters)
             removeWords(., c(stopwords(kind = "en"))),
# removing stopwords, which are words that are most common in the English language, yet carry no meaning. These are found inside the stopwords dictionary int he tm package
         title = title %>% tolower() %>% 
             gsub("[^[:alpha:]]", " ", .) %>% 
             gsub('\\b\\w{1,2}\\s|\\b\\w{21,}\\s','', .) %>% 
             removeWords(., c(stopwords(kind = "en"))))


```

---- **Tokenizing the data and creating bigrams** ----

> Here we will split the data word by word. We will do the analysis with unigrams (which is every word separated individually), and bigrams (which are words separated in twos). They may give us slightly different topics, though should be somewhat similar, so its important to check for overfitting if using both in a regression. 

```{r}
# Tokenizing

# Tokenizing with bigrams. What this does to us, is that as bigrams, words carry a different meaning as the paired words may carry a different meaning. 

DF <-  DF %>% 
  mutate(bigram_text = tokenize_ngrams(text , n = 2, ngram_delim = "_"),
         bigram_title = tokenize_ngrams(title , n = 2, ngram_delim = "_"),
         token_text = tokenize_ngrams(text , n = 1, ngram_delim = ""),
         token_title = tokenize_ngrams(title , n = 1, ngram_delim = ""))



```

---- *Adding the sentiment score* ----

> NOTE: adding the sentiment score takes a long time, (over 2 hours with 8 cores), the sentiment.rds file should have been included with this code, and it contains the data we get after we run the code. If you would prefer not to wait for the code to run, *skip the next chunk*. 

```{r, include=FALSE, message=FALSE,include=FALSE, eval=FALSE}

#Sentiment Analysis


# Calculating the sentiment score for each observation, this function may take some time to run. 


library(lexicon)
library(sentimentr)
library(parallel)

sentiment.fun  <- function(x){
  dictionary <- lexicon::hash_sentiment_sentiword
  out <- sentimentr::sentiment_by(sentimentr::sentiment(unlist(strsplit(x, split = " "))  ,polarity_dt = dictionary))
  
  out2 <- sum(out$ave_sentiment)/sum(out$word_count)
  return(out2)
}


timestart <- Sys.time() #checking start time


# Parallel processing to make it go quicker

sentiment<-as.vector(unlist(parallel::parLapply(cl, DF$token_text, sentiment.fun)))

Sys.time()-timestart->ptm  #checking how long it took the code to run


#The previous step takes a very long time to run (around 2 hours with 8 cores). Alternatively, you could just load the sentiment file, which has been run previously. 

#first save the sentiment if you have ran the file already.


saveRDS(sentiment, file = "sentiment.rds")

```



```{r}

sentiment <- readRDS(file = "sentiment.rds")

# Adding the sentiment score to our DF

DF$sentiment <-sentiment

# making sure there are no NA's left in the sentiment score. 

DF <- DF %>%
  na.omit(DF$sentiment)


```

---- *Adding more predictors based on our visualization* ----

```{r}

#Searching for certain keywords

#Politicians relevant at the time

politicians <- "(barack_obama)|(bernie_sanders)|(donald_trump)|(hillary_clinton)|(joe_biden)"  	
 

# Creating a politician per word metric, using the bigrams

DF$politician.pw <-
  str_count(string = paste(DF$bigram_text),
            pattern = politicians)/DF$w_count.txt

#Media related bigrams

media <- "(twitter_com)|(pic_twitter)|(getty_images)|(fox_news)|(social_media)|(fake_news)|(york_times)|(washington_post)|(mainstream_media)|(via_youtube)|(via_getty)|(via_video)|(facebook_page)|(youtube_com)|(abc_news)"

# Creating a politician per word metric, using the bigrams

DF$media.pw <-
  str_count(string = paste(DF$bigram_text),
            pattern = media)/DF$w_count.txt

```

### 2. Separate the data into training and test data.


```{r}

# Setting a seed so the experiment is replicable

set.seed(10)

# Creating test and train data:

sample <- initial_split(DF,
                           strata = Fake, # variable used for stratified sampling
                           prop = 0.75)# We are splitting 75% training and 25% test.

# Extracting the training data: 

train<-training(sample)

# Extracting the test data:

test<-testing(sample)

```



### 5. Training the machine learning model

```{r}

library(tidyverse)

registerDoSEQ()

# Select all variable we are using in our models

model_train <- train %>% 
  select(Fake,w_count.txt,punct_count.txt,exclamation_count.txt,w_count.title,punct_count.title, exclamation_count.title, sentiment, politician.pw, upper_count.title, media.pw, upper_count.txt  ) %>% 
  mutate(Fake = as.factor(Fake))

model_test <- test %>% 
  select(Fake,w_count.txt,punct_count.txt,exclamation_count.txt,w_count.title,punct_count.title, exclamation_count.title, sentiment, politician.pw, upper_count.title, media.pw, upper_count.txt ) %>% 
  mutate(Fake = as.factor(Fake))

```



```{r}

library(caret)

# logistic reg

set.seed(56)

glm_mod <- train(
  form = Fake ~ .,
  data = model_train,
  trControl = trainControl(method = "cv", number = 5),
  method = "glm",
  family = "binomial"
)

summary(glm_mod)


```


```{r}



# Variable importance plot

imp_lr <- varImp(glm_mod, scale = FALSE)

plot(imp_lr)


```


```{r}



# Making nice table to print

models <- list(glm_mod) # Making a list of all models

sum_table <- summary_table(models, model_test, model_test$Fake)

sum_table


```


```{r}

sum_table.bench

```


```{r}


# XGB

# Set up tuning parameters
xgb_grid <- expand.grid(nrounds = 2,
                        max_depth = c(6,7,8,10),
                        eta = c(0.1,0.2),
                        gamma = c(0,0.1),
                        colsample_bytree = 1,
                        min_child_weight = 1,
                        subsample = 1
)
set.seed(55)

# Train model

xgb_mod <- train(
  form = Fake ~.,
  data = model_train,
  trControl = trainControl(method = "cv", number = 5),
  method = "xgbTree",
  tuneGrid = xgb_grid,
  verbose = FALSE
)

# KNN

set.seed(55)

knn_mod <- train(
  form = Fake ~.,
  data = model_train,
  method = "knn",
  trControl = trainControl(method = "cv", number = 5),
  tuneGrid = expand.grid(k = seq(1,101, by = 2))
)

# GBM

gbm_grid = expand.grid(interaction.depth = c(2, 3, 4),
                       n.trees = c(200,400,600),
                       shrinkage = c(0.1, 0,2, 0.3),
                       n.minobsinnode = 15)

set.seed(55)

gbm_mod = train(
  form = Fake ~ .,
  data = model_train,
  trControl = trainControl(method = "cv", number = 5),
  method = "gbm",
  tuneGrid = gbm_grid, 
  verbose = FALSE
)



# Making nice table to print

models <- list(glm_mod,gbm_mod,knn_mod,xgb_mod) # Making a list of all models

# Make function

summary_table <- function(models,testdata, var_of_interest){
  
  # making empty matrix to fill values  
  
  sum_tab <- matrix(0,length(models),5)  
  
  colnames(sum_tab) <- c("Methods", "Accuracy", "Kappa", "Sensitivity", "Specificity") 
  
  # Loop over all models
  
  for(i in 1:length(models)){  
    
    # Predict and make a confusion matrix 
    
    pred <- predict(models[[i]], testdata)
    
    cm <- confusionMatrix(pred, var_of_interest)
    
    # make a nice data table of the result
    
    sum_tab[i,1]<- models[[i]]$method
    sum_tab[i,2]<- cm$overall[1]
    sum_tab[i,3]<- cm$overall[2]
    sum_tab[i,4]<- cm$byClass[1]
    sum_tab[i,5]<- cm$byClass[2]
    
  }
  
  sum_tab
  
}

sum_table <- summary_table(models, model_test, model_test$Fake)

kableExtra::kable(sum_table)



```


### 7. make a shiny app, which will work as an interface to plug in new data and see if the news stories is true or fake. 

```{r}

```


####  7.1 make the app more user friendly

```{r}

```


