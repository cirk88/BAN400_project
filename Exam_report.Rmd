---
title: "Group final project - BAN400 fall 2020 "
author: "Candidate: (...)"
date: "6/11/2020"
output:
  pdf_document:
    dev: png
  keep_tex: yes
  word_document: default
header-includes:
- \usepackage{dcolumn}
- \usepackage{booktabs}
- \usepackage{sectsty} \sectionfont{\centering}
- \usepackage{indentfirst}
fontsize: 11pt
spacing: double
indent: true
---


```{r setup, include=FALSE, message=FALSE}

knitr::opts_chunk$set(echo = FALSE)
options(scipen = 99999) # Do not wish to have scientific  notations.

# loading the required packages:

library(tidyverse)
library(parsnip)
library(yardstick)
library(rsample)
require(doParallel)
library(recipes)
library(ggridges)
library(ggplot2)
library(patchwork)
library(xgboost)
library(tidymodels)
library(FactoMineR)
library(tibble)
library(tm)
library(tidytext)
require(sentimentr)
library(SentimentAnalysis)
library(lexicon)
require(tokenizers)
require(wordcloud)

```


```{r}

### IMPORTANT! ###

#set the number of cores you will use for parallel calculation. Do not exceed the recommended amount.


CORES <- 8


cl <- parallel::makeCluster(CORES)

```

\tableofcontents

\pagebreak

# Project suggestion Group 30

## Initial proposal: 

*"Textual data analysis combined with regression analysis using data on fake/true news"*

## Method: 

- We create regressors based on the words present in the fake and real news.
 The regressors are based on both the formatting of the article, such as: 
*Number of words in the body and title of the article; Number of punctuation characters in the body and title; Number of exclamation marks in the body and title; Number of characters in upper case in the body and title*. And on the words present within the article, such as: *Words expressing positive or negative sentiment, words relating to relevant American politicians, words that relate to media itself*.   
 
- Then use a predictive model to see if the news are real or fake based on these factors. We run multiple models and choose the one with the best predictive score against the test dataset.

- Finally, we can create a "shiny ap" which allows up to paste news articles in, the ap will then preprocess the article and give us a score of "fake probability". 

## Analysis:

 - *Tokenizing and prepossessing before doing any textual analysis.* We need to prepossess the data, which mean shaping it in order for the different models to read them. Before that, we store the peculiarities of the text (such as the amount of punctiation characters present, for example).  

 - *Visualization of the data.* Since we are working with a large corpus (body of text), it is important to employ some techniques in order to better understand what the text we are using is talking about. We use frequency tables, wordclouds and KWIC (Keyword in context) tables to define what words are likely to carry information. 
 
 - *Sentiment analysis.* It is used to gather data on how negative /positive the fake news is compared with true news. Maybe there is a specific sentiment present which would add value to our analysis.
 
 - *Regression*, where 1 is fake news and 0 is true news. First we use a linear regression to understand the relationship between our regressors and our prediction. Then we run multiple machine learning algorithms, and we keep whichever performs best. 

## Sources

For this we will use a data set from Kaggle: https://www.kaggle.com/clmentbisaillon/fake-and-real-news-dataset 
(Links to an external site.) with data from 2016 to 2017.

All graphs will be visualized using ggplot package.
 
*Sentiment dictionary:* 
 
Baccianella S., Esuli, A. and Sebastiani, F. (2010). SentiWordNet 3.0: An Enhanced Lexical Resource for Sentiment Analysis and Opinion Mining. International Conference on Language Resources and Evaluation.

\pagebreak

## Workflow: 

### 1. getting the data to run in R and tidyverse

  
#### 1.1 Merging true and fake news datasets, adding a new dummy column if news is true or fake. 
  
```{r}

# Loading in the data, binding them together----

df_fake <- readRDS("Fake.rds") %>%
  mutate(Fake = 1)


df_true <- readRDS("True.rds") %>% 
  mutate(Fake = 0)


DF <- rbind(df_fake, df_true)

rm(df_true, df_fake)


```


#### 1.2 Prepossesing and getting the data ready for modelling. 

>

---- **Extracting counts of the formatting choices** ----

> Prepossesing the data means removing most of the formatting. However, we will use some formatting choices as regressors, so before shaping the text we need to extract the information we need. 

```{r}

# First we'd like to extract a few variables which we might use on our predictive models

DF <- DF %>% 
  mutate(w_count.txt = str_count(paste(DF$text), " "),
         punct_count.txt = str_count(paste(DF$text), "[[:punct:]]"),
         upper_count.txt = str_count(paste(DF$text), "[[:upper:]]"),
         exclamation_count.txt = str_count(paste(DF$text), "!"), 
         w_count.title = str_count(paste(DF$title), " "),
         punct_count.title = str_count(paste(DF$title), "[[:punct:]]"),
         upper_count.title = str_count(paste(DF$title), "[[:upper:]]"),
         exclamation_count.title = str_count(paste(DF$title), "!"))


```

>

---- *Prepossessing the data* ----

> Now we can preposses the data. In the case of our corpus, we: 

- Removed datapoints with no title, and bodies with less than 30 words.

- Removed uppercase letters.

- Removed any character that isn't alphabetical

- Removed any words that were shorter than 2 letters and larger than 21 letters

- Removed stopwords (words that are frequent but carry no meaning). 


```{r}

# Now we do some prepossessing of the data

DF <-  DF %>%
  filter( w_count.title > 0 ) %>%
  filter( w_count.txt > 30 ) %>% 
  mutate(text = text %>% tolower() %>%
             gsub("[^[:alpha:]]", " ", .) %>%
#this removes any character that's not alphabetic. Those usually not carry any meaning. We may want to analyse them individually.
             gsub('\\b\\w{1,2}\\s|\\b\\w{21,}\\s','', .) %>%
#removing words that are too small (from 1 or 2 letters and huge words, 21+ letters)
             removeWords(., c(stopwords(kind = "en"))),
# removing stopwords, which are words that are most common in the English language, yet carry no meaning. These are found inside the stopwords dictionary in the tm package
         title = title %>% tolower() %>% 
             gsub("[^[:alpha:]]", " ", .) %>% 
             gsub('\\b\\w{1,2}\\s|\\b\\w{21,}\\s','', .) %>% 
             removeWords(., c(stopwords(kind = "en"))))


```
>

\pagebreak

---- *Tokenizing the data and creating bigrams* ----


> Here we will split the data word by word. We will do the analysis with unigrams (which is every word separated individually), and bigrams (which are words separated in twos). The individual words will be used for the sentiment analysis, while bigrams will give us better keywords. 

```{r}
# Tokenizing

# Tokenizing with bigrams. What this does to us, is that as bigrams, words carry a different meaning as the paired words may carry a different meaning. 

DF <-  DF %>% 
  mutate(bigram_text = tokenize_ngrams(text , n = 2, ngram_delim = "_"),
         bigram_title = tokenize_ngrams(title , n = 2, ngram_delim = "_"),
         token_text = tokenize_ngrams(text , n = 1, ngram_delim = ""),
         token_title = tokenize_ngrams(title , n = 1, ngram_delim = ""))



```

>

---- *Adding the sentiment score* ----

> The code will run through every single word in our text corpus and rate them with a sentiment ranging from -1 (bad) to 1 (good). Then it will return the average score for each article.

> **NOTE:** Adding the sentiment score takes a long time, *over 2 hours with 8 cores*, the sentiment.rds file should have been included with this code, and it contains the data we get after we run the code. If you would prefer not to wait for the code to run, *skip the next chunk*. 

```{r, include=FALSE, message=FALSE,include=FALSE, eval=FALSE}

#Sentiment Analysis


# Calculating the sentiment score for each observation, this function may take some time to run. 


library(lexicon)
library(sentimentr)
library(parallel)

sentiment.fun  <- function(x){
  dictionary <- lexicon::hash_sentiment_loughran_mcdonald
  out <- sentimentr::sentiment_by(sentimentr::sentiment(unlist(strsplit(x, split = " "))  ,polarity_dt = dictionary))
  
  out2 <- sum(out$ave_sentiment)/sum(out$word_count)
  return(out2)
}


timestart <- Sys.time() #checking start time


# Parallel processing to make it go quicker

sentiment<-as.vector(unlist(parallel::parLapply(cl, DF$token_text, sentiment.fun)))

Sys.time()-timestart->ptm  #checking how long it took the code to run


#The previous step takes a very long time to run (around 2 hours with 8 cores). Alternatively, you could just load the sentiment file, which has been run previously. 

#first save the sentiment if you have ran the file already.


saveRDS(sentiment, file = "sentiment.rds")

```



```{r}

sentiment <- readRDS(file = "sentiment.rds")

# Adding the sentiment score to our DF

DF$sentiment <-sentiment

# making sure there are no NA's left in the sentiment score. 

DF <- DF %>%
  na.omit(DF$sentiment)


```

>

---- *Adding more predictors based on our visualization* ----

> We added this step after performing the visualization. Here we create a score based on the keyword topics we decided after looking at the data. We decided to create a politician per word score, and a media per word score. We decided that bigrams would carry more meaning in our analysis, and they were picked based on what we saw within the training data and what we believe to be relevant. This does mean that there is a higher potential for overfitting these predictors to the training data.

> The bigrams we chose for politicians were: 

- *barack_obama, bernie_sanders, donald_trump, hillary_clinton and joe_biden*

> The bigrams we chose for media were: 

- *twitter_com, pic_twitter, getty_images, fox_news, social_media, fake_news, york_times, washington_post, mainstream_media, via_youtube, via_getty, via_video, facebook_page, youtube_com and abc_news*



>
 
```{r}

#Searching for certain keywords

#Politicians relevant at the time

politicians <- "(barack_obama)|(bernie_sanders)|(donald_trump)|(hillary_clinton)|(joe_biden)"  	
 

# Creating a politician per word metric, using the bigrams

DF$politician.pw <-
  str_count(string = paste(DF$bigram_text),
            pattern = politicians)/DF$w_count.txt

#Media related bigrams

media <- "(twitter_com)|(pic_twitter)|(getty_images)|(fox_news)|(social_media)|(fake_news)|(york_times)|(washington_post)|(mainstream_media)|(via_youtube)|(via_getty)|(via_video)|(facebook_page)|(youtube_com)|(abc_news)"

# Creating a politician per word metric, using the bigrams

DF$media.pw <-
  str_count(string = paste(DF$bigram_text),
            pattern = media)/DF$w_count.txt

```


### 2. Separate the data into training and test data.


```{r}

# Setting a seed so the experiment is replicable

set.seed(10)

# Creating test and train data:

sample <- initial_split(DF,
                           strata = Fake, # variable used for stratified sampling
                           prop = 0.75)# We are splitting 75% training and 25% test.

# Extracting the training data: 

train<-training(sample)

# Extracting the test data:

test<-testing(sample)

```

>

> Here we split our data into training and test. First, we set a seed so the test is replicable. Then the split is made with 75% of our data in the training set, and 25% on the test set. The data is stratified with the Fake dummy variable in mind. It is important we go through this step before visualizing the data. Because we create some variables through visualization, and we don't want to introduce bias into our analysis. 

\pagebreak


### 3 Visualizing the data

> Here we have the steps we took to see how our data actually looked like. Since this data is filled with so much text, we used a Key words in context table to see specific words in the context of where they appear. We also can visuallize the frequency of the words in a wordcloud, to better understand what is being most spoke on in our dataset. 


```{r}

#splitting the data back into true and false to see the difference between them.

f.train <- train  %>% filter(train$Fake == 1)

t.train <- train  %>% filter(train$Fake == 0)


# extracting all the words from our training data to create the topic modeling

f.all_text <- paste(c(f.train$text), collapse = " ")

f.all_title <- paste(c(f.train$title), collapse = " ")

t.all_text <- paste(c(t.train$text), collapse = " ")

t.all_title <- paste(c(t.train$title), collapse = " ")

# splitting the words and tokenizing them.

f.text_tokens <- scan(text = f.all_text,
                      what = "character",
                      quote = "")

f.title_tokens <- scan(text = f.all_title,
                      what = "character",
                      quote = "")

f.text_bigrams <- paste(unlist(f.train$bigram_text), collapse = " ") %>% 
  strsplit(.," ") %>% unlist

f.title_bigrams <- paste(unlist(f.train$bigram_title), collapse = " ") %>% 
  strsplit(.," ") %>%  unlist

t.text_tokens <- scan(text = t.all_text,
                      what = "character",
                      quote = "")

t.title_tokens <- scan(text = t.all_title,
                      what = "character",
                      quote = "")

t.text_bigrams <- paste(unlist(t.train$bigram_text), collapse = " ") %>% 
  strsplit(.," ") %>% unlist

t.title_bigrams <- paste(unlist(t.train$bigram_title), collapse = " ") %>% 
  strsplit(.," ") %>%  unlist
  

```

#### 3.1 Visualizing the sentiment analysis


 > For the sentiment analysis, we decided to go with the sentiment dictionary sentiword. We chose that dictionary after looking at the words present in our data, and checking them in the KWIC matrix to see them in context. We ffound this dictionary in the lexicon package, and its description goes as follows: "*A data.table dataset containing an augmented version of Baccianella, Esuli and Sebastiani's (2010) positive/negative word list as sentiment lookup values.*"


```{r}

# Creating a dataframe with all the words

all_text <- paste(c(train$text), collapse = " ")


#tokenizing all words

text_tokens <- scan(text = all_text,
                      what = "character",
                      quote = "")

text_bigrams <- paste(unlist(train$bigram_text), collapse = " ") %>% 
  strsplit(.," ") %>% unlist

#Making a frequency table of all words

freq_text <- table(text_tokens)%>% 
  sort(decreasing = T) %>% 
  as.data.frame()

# choosing the sentiment dictionary 

dictionary <- lexicon::hash_sentiment_loughran_mcdonald
dictionary.n <- dictionary%>% filter(dictionary$y < 0)
dictionary.p <- dictionary%>% filter(dictionary$y > 0)

#seeing which positive words are present

positive_words <- freq_text %>%
  filter(text_tokens %in% dictionary.p$x) %>%
  arrange(desc(Freq)) %>% head(50)

# seeing which negative words are present

negative_words <- freq_text %>%
  filter(text_tokens %in% dictionary.n$x) %>%
  arrange(desc(Freq)) %>% head(50)


```

\pagebreak

---- *Positive words present in our data* ----

```{r,fig.height=4,fig.width=4,warning=FALSE,message=FALSE}

wordcloud(words = positive_words$text_tokens [1:50],
          freq = positive_words$Freq[1:50])

```
>

---- *Negative words present in our data* ----

```{r,fig.height=4,fig.width=4,warning=FALSE,message=FALSE}

wordcloud(words = negative_words$text_tokens [1:50],
          freq = negative_words$Freq[1:50])

```

\pagebreak

#### 3.2 Making a KWIC (Key words in context) table

> *this helps us better visualize the text and understand the context of some words*


```{r}


# Getting a KWIC matrix
# Here we build the function "get.KWIC" in order to inspect keywords. The KWIC matrix helps us identify keywords and the words related to it in our corpus. It aids in visualizing the data. 

### building the function ###

get.KWIC <- function(keyword,
                     n,
                     tokenized_text,
                     cores){
  cores <- min(detectCores(), cores)
  cl <- makeCluster(cores)
  registerDoParallel(cl)
  
  # this helps us seek the keyword
  index.env <- grep(keyword, tokenized_text)
  
  
  # this creates the table for us as a tibble. 
  KWIC <- tibble(left = parSapply(cl, index.env,
                               function(i) {ifelse( i-n >0,
                                 paste(tokenized_text[i-n:1],
                                                  collapse = " "),NA)}),
                 keyword = tokenized_text[index.env],
                 right = parSapply(cl, index.env,
                                function(i) {paste(tokenized_text[i+1:n],
                                                   collapse = " ")}))
  
  stopCluster(cl)
  # return
  return(KWIC)

}


```



```{r}


### using the function ###

get.KWIC(keyword = "trump",
         n = 3,
         tokenized_text = text_tokens, CORES) -> KWIC.trump

get.KWIC(keyword = "donald_trump",
         n = 2,
         tokenized_text = text_bigrams, CORES) -> KWIC.trump_bigram


## we just picked these topics since they are polarizing, but they are only here as an example of the function at work ##



```

---- *Example of keyword in context: trump on the tokenized data* ----

```{r}

KWIC.trump[4:10,]


```

---- *Example of keyword in context: donald_trump on the bigram data* ----

```{r}

KWIC.trump_bigram[4:10,]


```
\pagebreak

#### 3.3 Visualizing word frequency

```{r}

#making frequency tables out of the fake news

f.freq_text_1 <- table(f.text_tokens)%>% 
  sort(decreasing = T) %>% 
  as.data.frame()

f.freq_text_2 <- table(f.text_bigrams)%>% 
  sort(decreasing = T) %>% 
  as.data.frame()

f.freq_title_1 <- table(f.title_tokens)%>% 
  sort(decreasing = T) %>% 
  as.data.frame()

f.freq_title_2 <- table(f.title_bigrams)%>% 
  sort(decreasing = T) %>% 
  as.data.frame()

#making frequency tables out of the true news

t.freq_text_1 <- table(t.text_tokens)%>% 
  sort(decreasing = T) %>% 
  as.data.frame()

t.freq_text_2 <- table(t.text_bigrams)%>% 
  sort(decreasing = T) %>% 
  as.data.frame()

t.freq_title_1 <- table(t.title_tokens)%>% 
  sort(decreasing = T) %>% 
  as.data.frame()

t.freq_title_2 <- table(t.title_bigrams)%>% 
  sort(decreasing = T) %>% 
  as.data.frame()


```

> 

---- *Fake News word frequency* ----


```{r,fig.height=4,fig.width=4,warning=FALSE,message=FALSE}

#Making wordclouds of our training data


wordcloud(words = f.freq_text_1$f.text_tokens [1:50],
          freq = f.freq_text_1$Freq[1:50])

```

---- *True News word frequency* ----

```{r,fig.height=4,fig.width=4,warning=FALSE,message=FALSE}

wordcloud(words = t.freq_text_1$t.text_tokens [1:50],
          freq = t.freq_text_1$Freq[1:50])


```

\pagebreak

---- *Fake News bigram frequency* ----

```{r,fig.height=4,fig.width=4,warning=FALSE,message=FALSE}

wordcloud(words = f.freq_text_2$f.text_bigrams [1:50],
          freq = f.freq_text_2$Freq[1:50])

```

---- *True News bigram frequency* ----

```{r,fig.height=4,fig.width=4,warning=FALSE,message=FALSE}

wordcloud(words = t.freq_text_2$t.text_bigrams [1:50],
          freq = t.freq_text_2$Freq[1:50])

```
\pagebreak


### 4. Training the machine learning model

```{r,  message=FALSE, warning= F}

library(tidyverse)

registerDoSEQ()

# Select all variable we are using in our models

model_train <- train %>% 
  select(Fake,w_count.txt,punct_count.txt,exclamation_count.txt,w_count.title,punct_count.title, exclamation_count.title, sentiment, politician.pw, upper_count.title, media.pw, upper_count.txt  ) %>% 
  mutate(Fake = as.factor(Fake))

model_test <- test %>% 
  select(Fake,w_count.txt,punct_count.txt,exclamation_count.txt,w_count.title,punct_count.title, exclamation_count.title, sentiment, politician.pw, upper_count.title, media.pw, upper_count.txt ) %>% 
  mutate(Fake = as.factor(Fake))


```


```{r   message=FALSE, warning= F}

# Run logistic regression 

registerDoSEQ()

library(caret)

# logistic reg

set.seed(55)

glm_mod <- train(
  form = Fake ~ .,
  data = model_train,
  trControl = trainControl(method = "cv", number = 5),
  method = "glm",
  family = "binomial"
)


```


```{r, include=FALSE, message=FALSE,include=FALSE, eval=FALSE}
# All files are saved so that we do not need to run this script

# XGB

# Set up tuning parameters
xgb_grid <- expand.grid(nrounds = 2,
                        max_depth = c(6,7,8,10),
                        eta = c(0.1,0.2),
                        gamma = c(0,0.1),
                        colsample_bytree = 1,
                        min_child_weight = 1,
                        subsample = 1
)

set.seed(55)

# Train model

xgb_mod <- train(
  form = Fake ~.,
  data = model_train,
  trControl = trainControl(method = "cv", number = 5),
  method = "xgbTree",
  tuneGrid = xgb_grid,
  verbose = FALSE
)

# KNN

set.seed(55)

knn_mod <- train(
  form = Fake ~.,
  data = model_train,
  method = "knn",
  trControl = trainControl(method = "cv", number = 5),
  tuneGrid = expand.grid(k = seq(1,101, by = 2))
)

# GBM

gbm_grid = expand.grid(interaction.depth = c(2, 3, 4),
                       n.trees = c(200,400,600),
                       shrinkage = c(0.1, 0,2, 0.3),
                       n.minobsinnode = 15)

set.seed(55)

gbm_mod = train(
  form = Fake ~ .,
  data = model_train,
  trControl = trainControl(method = "cv", number = 5),
  method = "gbm",
  tuneGrid = gbm_grid, 
  verbose = FALSE
)



# Making nice table to print

models <- list(gbm_mod,knn_mod,xgb_mod) # Making a list of all models

saveRDS(models, "models.rds")

```

---- *Predictor importance plot* ----

```{r}

models <- c(list(glm_mod), readRDS("models.rds"))

# Variable importance plot

imp_lr <- varImp(models[[1]], scale = FALSE)

plot(imp_lr)


```


> Here we can see how well our predictors performed in the linear regression. It gives us a better understanding of how the data is shaped and what was most important on deciding if the news were fake or not. It seems like the formating of the data gives off most information. While the sentiment analysis, despite being statistically relevant, does not give much information. 

---- *Testing the trained regression models* ----

```{r}
# Make function

summary_table <- function(models,testdata, var_of_interest){
  
  # making empty matrix to fill values  
  
  sum_tab <- matrix(0,length(models),5)  
  
  colnames(sum_tab) <- c("Methods", "Accuracy", "Kappa", "Sensitivity", "Specificity") 
  
  # Loop over all models
  
  for(i in 1:length(models)){  
    
    # Predict and make a confusion matrix 
    
    pred <- predict(models[[i]], testdata)
    
    cm <- confusionMatrix(pred, var_of_interest)
    
    # make a nice data table of the result
    
    sum_tab[i,1]<- models[[i]]$method
    sum_tab[i,2]<- cm$overall[1]
    sum_tab[i,3]<- cm$overall[2]
    sum_tab[i,4]<- cm$byClass[1]
    sum_tab[i,5]<- cm$byClass[2]
    
  }
  
  sum_tab
  
}

sum_table <- summary_table(models, model_test, model_test$Fake)

kableExtra::kable(sum_table)


```

> Here we see that the GBM machine learning algorithm outperformed the other predictions. It also has a fairly high accuracy, albeit it is important to remember that this accuracy is very dependand on the fact that our test and trainig data comes from the same source. 

\pagebreak

### 5. make a shiny app, which will work as an interface to plug in new data and see if the news stories is true or fake. 

```{r}

```

